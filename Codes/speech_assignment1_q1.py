# -*- coding: utf-8 -*-
"""Speech_Assignment1_Q1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RljQsdcfsj1jGqxigKALm3YzKuMhWnsh
"""

!pip install torch torchaudio numpy pandas scikit-learn matplotlib seaborn tqdm

import os
import torch
import torchaudio
import pandas as pd
import numpy as np
import zipfile
import requests
import shutil
from torch.utils.data import Dataset, DataLoader
from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
import matplotlib.pyplot as plt
import seaborn as sns

# Define dataset URL and extraction path
DATASET_URL = "https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip?download=1"
DATASET_PATH = "ravdess_audio.zip"
EXTRACT_PATH = "ravdess_data"

# Download the dataset
def download_and_extract():
    if not os.path.exists(EXTRACT_PATH):
        print("Downloading dataset...")
        response = requests.get(DATASET_URL, stream=True)
        with open(DATASET_PATH, "wb") as file:
            shutil.copyfileobj(response.raw, file)

        print("Extracting dataset...")
        with zipfile.ZipFile(DATASET_PATH, 'r') as zip_ref:
            zip_ref.extractall(EXTRACT_PATH)
        print("Dataset downloaded and extracted.")
    else:
        print("Dataset already exists. Skipping download.")

download_and_extract()

# Function to get file paths and labels
def get_file_paths_and_labels():
    emotion_map = {
        '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',
        '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'
    }
    file_paths = []
    labels = []

    for root, _, files in os.walk(EXTRACT_PATH):
        for file in files:
            if file.endswith(".wav"):
                file_paths.append(os.path.join(root, file))
                emotion_code = file.split("-")[2]  # Extract emotion label
                labels.append(emotion_map[emotion_code])

    return file_paths, labels

file_paths, labels = get_file_paths_and_labels()

# Encode labels
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(labels)

# Compute class weights
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(encoded_labels), y=encoded_labels)
class_weights = torch.tensor(class_weights, dtype=torch.float)

# Split dataset
train_files, test_files, train_labels, test_labels = train_test_split(file_paths, encoded_labels, test_size=0.2, random_state=42)

# Load Wav2Vec 2.0 processor
# processor = Wav2Vec2Processor.from_pretrained("superb/wav2vec2-base-superb-er")
processor = Wav2Vec2Processor.from_pretrained("audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim")

# Define custom dataset
class EmotionDataset(Dataset):
    def __init__(self, file_paths, labels, processor, target_sample_rate=16000, max_length=16000*3):
        self.file_paths = file_paths
        self.labels = labels
        self.processor = processor
        self.target_sample_rate = target_sample_rate
        self.max_length = max_length

    def __len__(self):
        return len(self.file_paths)

    def __getitem__(self, idx):
        waveform, sample_rate = torchaudio.load(self.file_paths[idx])

        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)

        if sample_rate != self.target_sample_rate:
            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.target_sample_rate)
            waveform = resampler(waveform)

        num_samples = waveform.shape[1]
        if num_samples > self.max_length:
            waveform = waveform[:, :self.max_length]
        else:
            pad_amount = self.max_length - num_samples
            waveform = torch.nn.functional.pad(waveform, (0, pad_amount))

        waveform = waveform.squeeze(0)

        inputs = self.processor(
            waveform.numpy(),
            sampling_rate=self.target_sample_rate,
            return_tensors="pt",
            padding=True
        )
        return inputs.input_values.squeeze(0), torch.tensor(self.labels[idx])

# Create data loaders
train_dataset = EmotionDataset(train_files, train_labels, processor)
test_dataset = EmotionDataset(test_files, test_labels, processor)
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

# Load pre-trained Wav2Vec 2.0 model
model = Wav2Vec2ForSequenceClassification.from_pretrained("audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim", num_labels=len(label_encoder.classes_))

# Define optimizer and loss function
optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)
criterion = torch.nn.CrossEntropyLoss(weight=class_weights.to(torch.device("cuda" if torch.cuda.is_available() else "cpu")))

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# Training loop
def train_model(model, train_loader, optimizer, criterion, epochs=30):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs).logits
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}")

train_model(model, train_loader, optimizer, criterion)

# Save model
torch.save(model.state_dict(), "emotion_recognition_model.pth")
print("Model training complete and saved.")

# Evaluation function
def evaluate_model(model, test_loader):
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs = inputs.to(device)
            outputs = model(inputs).logits
            preds = torch.argmax(outputs, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.numpy())

    print("Accuracy:", accuracy_score(all_labels, all_preds))
    print("Classification Report:")
    print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))

    # Plot confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(8,6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title("Confusion Matrix")
    plt.show()

evaluate_model(model, test_loader)

